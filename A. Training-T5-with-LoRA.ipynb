{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwVPn3XgCkXs"
      },
      "outputs": [],
      "source": [
        "# 필요 라이브러리 설치\n",
        "!pip install \"peft\"\n",
        "!pip install \"transformers==4.30\" \"datasets==2.9.0\" \"accelerate\" \"evaluate==0.4.0\" \"bitsandbytes\" loralib --upgrade --quiet\n",
        "!pip install rouge-score tensorboard py7zr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset\n",
        "\n",
        "- samsum dataset : 16k messenger-like conversations with summaries."
      ],
      "metadata": {
        "id": "XcStP9_RFgEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"samsum\")\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "id": "Y2OlZAcHDecm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = \"google/flan-t5-xxl\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "ds-eYDQgRRBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습하기 전, 적절한 max_length 설정을 위한 데이터 탐색\n",
        "\n"
      ],
      "metadata": {
        "id": "Vax4Jk_mJTyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "import numpy as np\n",
        "\n",
        "tokenized_inputs = concatenate_datasets([dataset['train'], dataset['test']]).map(\n",
        "                                                                                    lambda x: tokenizer(x['dialogue'], truncation=True),\n",
        "                                                                                    batched=True,\n",
        "                                                                                    remove_columns=['dialogue', 'summary']\n",
        "                                                                                )\n",
        "input_lengths = [len(x) for x in tokenized_inputs['input_ids']]\n",
        "# 대화문의 최대 길이의 85분위 사용\n",
        "max_source_length = int(np.percentile(input_lengths, 85))\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "\n",
        "tokenized_inputs = concatenate_datasets([dataset['train'], dataset['test']]).map(\n",
        "                                                                                    lambda x: tokenizer(x['summary'], truncation=True),\n",
        "                                                                                    batched=True,\n",
        "                                                                                    remove_columns=['dialogue', 'summary']\n",
        "                                                                                )\n",
        "target_lengths = [len(x) for x in tokenized_inputs['input_ids']]\n",
        "# 요약문의 최대 길이의 90분위 사용\n",
        "max_target_length = int(np.percentile(target_lengths, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ],
      "metadata": {
        "id": "L6EYRvJ1KJ2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prerocess function 정의"
      ],
      "metadata": {
        "id": "f0nIan72SwUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(sample, padding=\"max_length\"):\n",
        "    # T5 모델을 위한 input 변경\n",
        "    print(sample)\n",
        "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # Tokenize input\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize target(text_target 사용)\n",
        "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "\n",
        "    # padding 토큰이 loss 값을 계산할 때 무시되기 위해서는 pad_token_id가 아닌 -100으로 바꿔주어야 함\n",
        "    if padding == \"max_length\":\n",
        "        labels['input_ids'] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
        "        ]\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function,\n",
        "                                batched=True,\n",
        "                                remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "\n",
        "\n",
        "print(f\"Columns of tokenized dataset {list(tokenized_dataset['train'].features)}\")\n",
        "\n",
        "# 쉽게 불러올 수 있도록 디스크에 저장\n",
        "tokenized_dataset['train'].save_to_disk('./data/train')\n",
        "tokenized_dataset['test'].save_to_disk('./data/eval')"
      ],
      "metadata": {
        "id": "PbxWU91VTM16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA를 이용한 T5의 Fine-tuning\n",
        "\n",
        "[bitsanbytes.LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)을 이용해 LLM을 int8로 quantize할 수 있으며 이 방식을 통해 메모리를 크게 줄일 수 있다.\n",
        "\n",
        "> Google Colab에서 *load_in_8bit*인자의 GPU 용량 문제로 인해 *load_in_4bit*로 대체\n"
      ],
      "metadata": {
        "id": "AT8SO3-adE5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "qh_UQbxUXuXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k-bit 학습 준비\n",
        "\n",
        "- 결과적으로 전체 모델의 약 0.16%의 파라미터만을 학습하게 된다.\n",
        "\n",
        "- 따라서, 메모리 문제 없이 모델을 fine-tuning 할 수 있게 된다."
      ],
      "metadata": {
        "id": "JDfFBsobHqv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Lora Config 정의\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "# 모델 학습 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA adaptor 추가\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "kMNzHq1mtsZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataCollator\n",
        "\n",
        "- 동적 padding을 위한 객체"
      ],
      "metadata": {
        "id": "X5jD1YkhIA2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "label_pad_token_id = -100\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8 # 해당 인자의 배수로 padding이 채워짐.(대체적으로 8의 배수가 좋다고 함)\n",
        ")"
      ],
      "metadata": {
        "id": "UbG8v2vtIGLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "output_dir = 'lora-flan-t5-xxl'\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Trainer 생성\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        ")\n",
        "\n",
        "# True인 경우 과거 키 값을 사용, 훈련 과정에서는 필요하지 않음\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "Hf1g0EagIj3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5의 경우 몇몇 layer들은 안정성을 위해 *float32*로 유지됨"
      ],
      "metadata": {
        "id": "xyzoHDs-KlTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "j9BOWRCdKilK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "trainer.model.save_pretrained('results')\n",
        "tokenizer.save_pretrained('results')"
      ],
      "metadata": {
        "id": "Oexn3EehR3U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA FLAN-T5를 사용한 평가 및 추론\n",
        "\n",
        "- 저장된 모델의 크기는 약 70M에 불과함"
      ],
      "metadata": {
        "id": "zd4_IKlZSPT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# PEFT config 불러오기\n",
        "peft_model_id = \"results\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "# Base Model, Tokenizer 불러오기\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_4bit=True,  device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# LoRA 모델 불러오기\n",
        "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "model.eval()\n",
        "\n",
        "print(\"Peft model loaded\")"
      ],
      "metadata": {
        "id": "EqteAvt6L7zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample을 이용한 추론 테스트"
      ],
      "metadata": {
        "id": "Cz4_iV2aWFWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from random import randrange\n",
        "\n",
        "\n",
        "# 데이터 불러오기 및 샘플링\n",
        "dataset = load_dataset(\"samsum\")\n",
        "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
        "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "# 모델 추론 결과\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
        "\n",
        "\n",
        "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
        "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
      ],
      "metadata": {
        "id": "baOMJkTIVkCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 평가"
      ],
      "metadata": {
        "id": "BL8v_VH_Wsm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시간이 많이 소요되어 10개의 데이터만 사용\n",
        "from datasets import Dataset\n",
        "\n",
        "test_dataset = Dataset.from_dict(test_dataset[:10])"
      ],
      "metadata": {
        "id": "3UtykkqpZCxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_from_disk\n",
        "from tqdm import tqdm\n",
        "\n",
        "metric = evaluate.load('rouge')\n",
        "\n",
        "def evaluate_peft_model(sample, max_target_length=50):\n",
        "  # 요약 결과\n",
        "  outputs = model.generate(input_ids=sample['input_ids'].unsqueeze(0).cuda(),\n",
        "                           do_sample=True,\n",
        "                           top_p=0.9,\n",
        "                           max_new_tokens=max_target_length,\n",
        "                           )\n",
        "\n",
        "  prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
        "\n",
        "  # -100 토큰 pad 토큰으로 변경 및 디코딩\n",
        "  labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
        "  labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  return prediction, labels\n",
        "\n",
        "# 테스트 데이터 불러오기(시간이 많이 소요되어 10개의 데이터만 사용)\n",
        "test_dataset = load_from_disk('data/eval/').with_format(\"torch\")\n",
        "test_dataset = Dataset.from_dict(test_dataset[:10]).with_format(\"torch\")\n",
        "\n",
        "\n",
        "# 추론 시작\n",
        "predictions, references = [], []\n",
        "for sample in tqdm(test_dataset):\n",
        "  p, l = evaluate_peft_model(sample)\n",
        "  predictions.append(p)\n",
        "  references.append(l)\n",
        "\n",
        "# 계산\n",
        "rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "print(f\"Rouge1: {rouge['rouge1']* 100:2f}%\")\n",
        "print(f\"Rouge2: {rouge['rouge2']* 100:2f}%\")\n",
        "print(f\"RougeL: {rouge['rougeL']* 100:2f}%\")\n",
        "print(f\"RougeLsum: {rouge['rougeLsum']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "pcEqU6P_WuHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab으로 학습을 제대로 진행하지 못했음에도 불구하고 rouge1은 약 38%의 평가를 보여줌."
      ],
      "metadata": {
        "id": "lO5wJZmcZ7yp"
      }
    }
  ]
}
